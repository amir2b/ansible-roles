---

#################### Install java ####################

# sudo apt install -y default-jre
- name: Install java
  tags: [ setup, packages ]
  become: yes
  apt:
    name: default-jre
    update_cache: yes

#################### Create hadoop user ####################

# sudo groupadd hadoop
- name: Add the group 'hadoop'
  tags: [ setup, packages ]
  become: yes
  user:
    name: hadoop

# sudo adduser hadoop
# ssh-keygen -t rsa
- name: Add the user 'hadoop'
  tags: [ setup, packages ]
  become: yes
  user:
    name: hadoop
    group: hadoop
    shell: /bin/bash
    generate_ssh_key: yes
    comment: Hadoop

#################### Install spark ####################

- name: Check that the spark exists
  tags: [ setup, packages ]
  stat:
    path: /opt/spark
  register: stat_spark

- name: Install spark
  when: stat_spark.stat.isdir is not defined
  block:

    # wget https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
    - name: Download spark {{ spark_version }} archive file
      tags: [ setup, packages ]
      become: yes
      get_url:
        url: https://downloads.apache.org/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }}.tgz
        dest: /home/hadoop/spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }}.tgz
        owner: hadoop
        group: hadoop

    # tar -xzf spark-3.1.2-bin-hadoop3.2.tgz
    - name: Extract spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }}.tgz into /opt/
      tags: [ setup, packages ]
      become: yes
      unarchive:
        src: /home/hadoop/spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }}.tgz
        dest: /opt/
        remote_src: yes
        owner: hadoop
        group: hadoop

    # sudo mv /opt/spark-3.1.2-bin-hadoop3.2 /opt/spark
    - name: Rename spark directory
      tags: [ setup, packages ]
      become: yes
      command: mv "/opt/spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }}" /opt/spark

    - name: Remvoe all cmd files
      tags: [ setup, packages ]
      become: yes
      file:
        path: "/opt/spark/{{ item }}"
        state: absent
      with_items:
        - bin/beeline.cmd
        - bin/find-spark-home.cmd
        - bin/load-spark-env.cmd
        - bin/pyspark2.cmd
        - bin/pyspark.cmd
        - bin/run-example.cmd
        - bin/spark-class2.cmd
        - bin/spark-class.cmd
        - bin/sparkR2.cmd
        - bin/sparkR.cmd
        - bin/spark-shell2.cmd
        - bin/spark-shell.cmd
        - bin/spark-sql2.cmd
        - bin/spark-sql.cmd
        - bin/spark-submit2.cmd
        - bin/spark-submit.cmd

#################### Install hadoop ####################

- name: Check that the hadoop exists
  tags: [ setup, packages ]
  stat:
    path: /opt/hadoop
  register: stat_hadoop

- name: Install Hadoop
  when: stat_hadoop.stat.isdir is not defined
  block:

    # wget https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz
    - name: Download hadoop {{ hadoop_version }} archive file
      tags: [ setup, packages ]
      become: yes
      get_url:
        url: https://downloads.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz
        dest: /home/hadoop/hadoop-{{ hadoop_version }}.tar.gz
        owner: hadoop
        group: hadoop

    # tar xzf hadoop-3.3.1.tar.gz
    - name: Extract hadoop-{{ hadoop_version }}.tar.gz into /opt/
      tags: [ setup, packages ]
      become: yes
      unarchive:
        src: /home/hadoop/hadoop-{{ hadoop_version }}.tar.gz
        dest: /opt/
        remote_src: yes
        owner: hadoop
        group: hadoop

    # sudo mv /opt/hadoop-3.3.1 /opt/hadoop
    - name: Rename hadoop directory
      tags: [ setup, packages ]
      become: yes
      command: mv "/opt/hadoop-{{ hadoop_version }}" /opt/hadoop

    - name: Remvoe all cmd files
      tags: [ setup, packages ]
      become: yes
      file:
        path: "/opt/hadoop/{{ item }}"
        state: absent
      with_items:
        - bin/hadoop.cmd
        - bin/hdfs.cmd
        - bin/mapred.cmd
        - bin/yarn.cmd
        - sbin/start-all.cmd
        - sbin/start-dfs.cmd
        - sbin/start-yarn.cmd
        - sbin/stop-all.cmd
        - sbin/stop-dfs.cmd
        - sbin/stop-yarn.cmd
